# full-link-stress-test
全链路压测

# 背景

　　2012年双十一零点的时候，全国有数千万人同时登录淘宝网，几十万人同时在一秒钟浏览商品、创建订单、支付款项。巨大的流量压力导致系统出错，订单被重复支付导致商品超卖，还有优惠券不生效导致商家与消费者怨声载道，用户投诉电话接不过来，甚至都上了央视新闻，这件事情给阿里带来了不小的负面影响。

　　2013年阿里云提出了通过影子表、影子库等技术来实现全链路压测的概念，对集团的所有的中间件、核心应用针对性的做了改造，使其具备支撑全链路压测的能力。 影子表能力可以让压测产生的写入数据全部都隔离到其他区域，不影响生产数据，应用升级中间件后就自动具备了这个能力。为了解决商业压测工具不能提供超大瞬间流量的问题，阿里自研了压测流量产品，通过分布在全国各地的CDN机器，同时发起超大并发流量，对集团内部应用进行全链路压测。通过模拟双十一相同的生产集群、流量模型、流量规模的方案，来提前验证系统是否具备支撑双十一的高压能力，从而保障了阿里双十一的稳定运行。
  
  
# 为什么选择线上环境而不是测试环境

　　理想情况下， 如果测试环境想跟线上环境完全一致，软硬件都一致，是可以做到的。 但是为了保持完全一致，机器成本、维护成本、数据同步都需要大量的支出，所以这也要根据公司情况来衡量。如果用线上环境直接做压测的话就不需要机器与维护成本了，但是难点在于对技术的要求比较高。

# 全链路压测 的应用场景

   新系统上线场景：准确探知系统承载能力，防止刚上线被用户流量冲垮。
   
# 全链路压测 好处
   
   1保障重大活动的系统稳定性

　　避免公司业务和声誉因为技术故障受到损失，为技术团队赢得业务团队的尊重。

　　2 精准的容量评估

　　帮助公司用最低的成本满足业务的性能要求

　　3 重大项目重构切换前的性能验证

　　系统重构是IT部门场景的技术更新的方法，每次上线都需要经历一段阵痛期，期间性能问题、业务故障频发，用户投诉频繁。通过全链路压测可以在正式切换前完全解决性能问题；配合自动化的用例梳理和人工验证，可以极大程度降低业务故障。两者配合使用，可以快速的渡过不稳定期，提升用户体验。

　　4 端到端的全链路巡检，第一时间发现故障并快递定位问题

　　常见的监控体系通过一些间接的指标来判断是否有故障发生（比如通过CPU利用率、内存使用率、应用的错误日志数量、业务单量和基线的对比等等方式），间接的方式会产生大量的误报，造成告警麻痹症，真的故障发生后不一定能第一时间引起重视。

　　通过全链路压测提供的数据隔离功能，可以在线上通过压测流量验证真实的业务接口是否能正常工作。这种方式可以直接在用户发现业务故障前，让相关人员第一时间知晓。配合链路的监测分析功能，快速定位问题应用所在。经过验证该方法在客户真实环境中比传统监控方法平均提前7分钟发现故障，告警正确率是传统告警方式的几十倍。

　　5 建立公司的性能运营体系，将运动式的性能优化演化为自发的日常性能优化

　　很多公司都有运动式或者故障驱动的性能优化经历，比如马上要双十一，总监牵头开始性能优化；有人管的时候性能表现很好，一旦没人牵头做性能优化的事情，又会有很多性能问题被暴露出来。这样的方式通过优化效率很低，投入还大。

　　6 通过全链路压测的方式，配合目标制定、绩效和工单系统。

　　自动化的全链路压测可以日常化的排查性能瓶颈，通过工单将问题直达负责人，极大的提升性能优化的效率，将性能问题控制在萌芽状态。
   
   
  
  
